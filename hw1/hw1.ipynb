{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bXoQir0P3P_L"
   },
   "source": [
    "# 10-714: Homework 1\n",
    "\n",
    "This homework will get you started with your implementation of the **needle** (**ne**cessary **e**lements of **d**eep **le**arning) library.  In particular, the goal of this assignment is to build a basic **automatic differentiation** framework, then use this to re-implement the simple two-layer neural network you used for the MNIST digit classification problem in HW0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to set up the assignment\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd /content/drive/MyDrive/\n",
    "# !mkdir -p 10714\n",
    "# %cd /content/drive/MyDrive/10714\n",
    "# !git clone https://github.com/dlsys10714/hw1.git\n",
    "# %cd /content/drive/MyDrive/10714/hw1\n",
    "\n",
    "# !pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "# !pip3 install numdifftools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9vtSr-B3RMx",
    "outputId": "10894b4a-cced-49c4-917b-73cc51a07d30"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "sys.path.append('./apps')\n",
    "from simple_ml import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to `needle`\n",
    "\n",
    "For this homework, you will be implementing the basics of automatic differentiation using a `numpy` CPU backend (in later assignments, you will move to your own linear algebra library including GPU code). All code for this assignment will be written in Python.\n",
    "\n",
    "For the purposes of this assignment, there are two important files in the `needle` library, the `python/needle/autograd.py` file (which defines the basics of the computational graph framework, and also will form the basis of the automatic differentiation framework), and the `python/needle/ops.py`.file (which contains implementations of various operators).\n",
    "\n",
    "- `Value`: A value computed in a compute graph, i.e., either the output of some operations applied to other `Value` objects, or a constant (leaf) `Value` objects.  We use a generic class here (which we then specialize to e.g. Tensors), in order to allow for other data structures in later version of needle, but for now you will interact with this class mostly through its subclass `Tensor`.\n",
    "- `Op`: An operator in a compute graph.  Operators need to define their \"forward\" pass in the `compute()` method (i.e., how to compute the operator on the underlying data of the `Value` objects), as well as their \"backward\" pass via the `gradient()` method, which defines how to multiply by incoming output gradients.\n",
    "- `Tensor`: This is a subclass of `Value` that corresponds to an actual tensor output, i.e., a multi-dimensional array within a computation graph.  All of your code for this assignment (and most of the following) will use this subclass of `Value` rather than the generic class above.  We have provided several convenience functions (e.g., operator overloading) that let you operate on tensor using normal Python conventions, but these will not work properly until you implement the corresponding operations.\n",
    "- `TensorOp`: This is a subclass for `Op` for operators that return Tensors.  All the operations you implement for this assignment will be of this type.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "18KCMTWT3zVY"
   },
   "source": [
    "## Question 1: Implementing forward computation [10 pts]\n",
    "\n",
    "\n",
    "First, you will implement the forward computation for new operators.  To see how this works, consider the `EWiseAdd` operator in the `ops.py` file.\n",
    "\n",
    "The `compute()` function computes the \"forward\" pass, i.e., just computes the operation itself.  However, it is important to emphasize the inputs to compute are both `NDArray` objects.  That is, `compute()` computes the forward pass on the _raw data objects_ themselves, not on Tensor objects within the automatic differentiation (manipulation on `Tensor` objects might change the compuete graph).\n",
    "\n",
    "We will discuss the `gradient()` call in the next section, but it is important to emphasize here that this call is different from forward in that it takes `Tensor` arguments (instead of _raw data objects_).  This means that any call you make within this function _should_ be done via `TensorOp` operations themselves (so that you can take gradients of gradients).\n",
    "\n",
    "Finally, note that we also define a helper `add()` function, to avoid the need to call `EWiseAdd()(a,b)` (which is a bit cumbersome) to add two `Tensor` objects.  These functions are all written for you, and should be self-explanatory.\n",
    "\n",
    "For this question, you will need to implement the `compute` call for each of the following classes.  These calls are very straightforward, and should be essentially one line that calls to the relevant numpy function.  Note that because in later homeworks you will use a backend other than numpy, we have imported numpy as `import numpy as array_api`, so that you'll need to call `array_api.add()` etc, if you want to use the typical `np.X()` calls.\n",
    "\n",
    "- `PowerScalar`\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `MatMul`\n",
    "- `Summation`\n",
    "- `BroadcastTo`\n",
    "- `Reshape`\n",
    "- `Negate`\n",
    "- `Transpose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.15, pytest-7.2.0, pluggy-1.0.0 -- /home/hujunhao/.conda/envs/dlsys/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/hujunhao/dlsys/hw1\n",
      "collected 21 items / 13 deselected / 8 selected                                \u001b[0m\n",
      "\n",
      "tests/test_autograd_hw.py::test_divide_forward \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 12%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_divide_scalar_forward \u001b[32mPASSED\u001b[0m\u001b[32m             [ 25%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_matmul_forward \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 37%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_summation_forward \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 50%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_broadcast_to_forward \u001b[32mPASSED\u001b[0m\u001b[32m              [ 62%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_reshape_forward \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 75%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_negate_forward \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 87%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_transpose_forward \u001b[32mPASSED\u001b[0m\u001b[32m                 [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m8 passed\u001b[0m, \u001b[33m13 deselected\u001b[0m\u001b[32m in 0.20s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -v -k \"forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Implementing backward computation [25 pts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to perform these computations is, again, via taking \"fake\" partial derivatives (assuming everything is a scalar), and then matching sizes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing backward passes\n",
    "\n",
    "Note that, unlike the forward pass functions, the arguments to the `gradient` function are `needle` objects. It is important to implement the backward passes using only `needle` operations (i.e. those defined in `python/needle/ops.py`), rather than using `numpy` operations on the underlying `numpy` data, so that we can construct the gradients themselves via a computation graph (one exception is for the `ReLU` operation defined below, where you could directly access data within the Tensor without risk because the gradient itself is non-differentiable, but this is a special case).\n",
    "\n",
    "\n",
    "To complete this question, fill in the `gradient` function of the following classes:\n",
    "\n",
    "- `EWiseDiv`\n",
    "- `DivScalar`\n",
    "- `MatMul`\n",
    "- `Summation`\n",
    "- `BroadcastTo`\n",
    "- `Reshape`\n",
    "- `Negate`\n",
    "- `Transpose`\n",
    "\n",
    "**Hint:** Remember that the size of `out_grad` will always be the size of the _output_ of the operation, whereas the sizes of the `Tensor` objects _returned_ by `gradient()` have to always be the same as the original _inputs_ to the operator.\n",
    "\n",
    "\n",
    "### Checking backward passes\n",
    "To reiterate the above, remember that we can check that these backward passes are correct by doing numerical gradient checking as covered in lecture:\n",
    "\\begin{equation}\n",
    "\\delta^T \\nabla_\\theta f(\\theta) = \\frac{f(\\theta + \\epsilon \\delta) - f(\\theta - \\epsilon \\delta)}{2 \\epsilon} + o(\\epsilon^2)\n",
    "\\end{equation}\n",
    "We provide the function `gradient_check` for doing this numerical checking in `tests/test_autograd.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etSSMzD0fee0",
    "outputId": "2e8d28f1-9dee-4ad0-d9c3-9d9bdb1c30ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.15, pytest-7.2.0, pluggy-1.0.0 -- /home/hujunhao/.conda/envs/dlsys/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/hujunhao/dlsys/hw1\n",
      "collected 21 items / 12 deselected / 9 selected                                \u001b[0m\n",
      "\n",
      "tests/test_autograd_hw.py::test_divide_backward \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 11%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_divide_scalar_backward \u001b[32mPASSED\u001b[0m\u001b[32m            [ 22%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_matmul_simple_backward \u001b[32mPASSED\u001b[0m\u001b[32m            [ 33%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_matmul_batched_backward \u001b[32mPASSED\u001b[0m\u001b[32m           [ 44%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_reshape_backward \u001b[32mPASSED\u001b[0m\u001b[32m                  [ 55%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_negate_backward \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 66%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_transpose_backward \u001b[32mPASSED\u001b[0m\u001b[32m                [ 77%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_broadcast_to_backward \u001b[32mPASSED\u001b[0m\u001b[32m             [ 88%]\u001b[0m\n",
      "tests/test_autograd_hw.py::test_summation_backward \u001b[32mPASSED\u001b[0m\u001b[32m                [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m9 passed\u001b[0m, \u001b[33m12 deselected\u001b[0m\u001b[32m in 0.32s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -v -k \"backward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"backward\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SQMdWR1x7Euc"
   },
   "source": [
    "## Question 3: Topological sort [20 pts]\n",
    "\n",
    "Now your system is capable of performing operations on tensors which builds up a computation graph. Next you will write one of the main utilities needed for automatic differentiation - the [topological sort](https://en.wikipedia.org/wiki/Topological_sorting). This will allow us to traverse through (forward or backward) the computation graph, computing gradients along the way.\n",
    "\n",
    "Fill out the `find_topo_sort` method and the `topo_sort_dfs` helper method (in `python/needle/autograd.py`) to perform this topological sorting. \n",
    "\n",
    "#### Hints: \n",
    "- Ensure that you do a post-order depth-first search, otherwise the test cases will fail. \n",
    "- The `topo_sort_dfs` method is not required, but we find it useful to use this as a recursive helper function. \n",
    "- The \"Reverse mode AD by extending computational graph\" section of the Lecture 4 slides walks through an example of the proper node ordering. \n",
    "- We will be traversing this sorting backwards in later parts of this homework, but the `find_topo_sort` should return the node ordering in the forward direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQqntvth8GVa",
    "outputId": "c8bcdac7-c8e3-47ca-f450-fac21d29bfab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.15, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/hujunhao/dlsys/hw1\n",
      "collected 21 items / 20 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_autograd_hw.py \u001b[32m.\u001b[0m\u001b[32m                                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 0.22s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -k \"topo_sort\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rPouXyxCfee1",
    "outputId": "803faa1b-a480-4501-8149-b32ff4d1b90b"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"topo_sort\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OTDpkG098JiR"
   },
   "source": [
    "## Question 4: Implementing reverse mode differentiation [25 pts]\n",
    "\n",
    "Once you have correctly implemented the topological sort, you will next leverage it to implement reverse mode automatic differentiation. As a recap from last lecture, we will need to traverse the computational graph in reverse topological order, and construct the new adjoint nodes. For this question, implement the Reverse AD algorithm in the `compute_gradient_of_variables` function in `python/needle/autograd.py`. This will enable use of the `backward` function that computes the gradient and stores the gradient in the `grad` field of each input `Tensor`. With this completed, our reverse mode auto-differentiation engine is functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in lecture the result of reverse mode AD is still a computational graph. We can extend that graph further by composing more operations and run reverse mode AD again on the gradient (the last two tests of this problem). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L37C7ez8fkM",
    "outputId": "f2846195-0754-465c-97d7-c36a7ade8cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.15, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/hujunhao/dlsys/hw1\n",
      "collected 21 items / 20 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_autograd_hw.py \u001b[32m.\u001b[0m\u001b[32m                                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 0.23s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -k \"compute_gradient\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwjSL3n6fee2",
    "outputId": "8ff6a756-ac4d-4255-bef0-181c7ebbb594"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"compute_gradient\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qgMkRkJE8i9D"
   },
   "source": [
    "## Question 5: Softmax loss [10 pts]\n",
    "\n",
    "In this question, you will implement the softmax loss as defined in the `softmax_loss()` function in `apps/simple_ml.py`, which we defined in Question 3 of Homework 0, except this time, the softmax loss takes as input a `Tensor` of logits and a `Tensor` of one hot encodings of the true labels. \n",
    "\n",
    "Finally, note that the average softmax loss returned should also be a `Tensor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CpwK-c989OkI",
    "outputId": "0f145d00-6dce-46c9-d5f4-5d72e6d65419"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.15, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/hujunhao/dlsys/hw1\n",
      "collected 21 items / 20 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_autograd_hw.py \u001b[32m.\u001b[0m\u001b[32m                                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 0.59s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -k \"softmax_loss_ndl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DTwK4X6bfee2",
    "outputId": "6e666bd5-7f75-461b-b42a-673512ba953d"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"softmax_loss_ndl\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oWGd-mr-9luT"
   },
   "source": [
    "## Question 6: SGD for a two-layer neural network [10 pts]\n",
    "\n",
    "First, you will need to implement the forward and backward passes of the `relu` operator. \n",
    "1. Begin by filling out the function `ReLU` operator in `python/needle/ops.py`.\n",
    "2. Then fill out the `gradient` function of the class `ReLU` in `python/needle/ops.py`.  **Note that in this one case it's acceptable to access the `.realize_cached_data()` call on the output tensor, since the ReLU function is not twice differentiable anyway**.\n",
    "\n",
    "Then, \n",
    "\n",
    "3. Fill out the `nn_epoch` method in the `apps/simple_ml.py` file. \n",
    "\n",
    "Note that unlike in Homework 0, the inputs `W1` and `W2` are `Tensors`. Inputs `X` and `y` however are still numpy arrays - you should iterate over mini-batches of the numpy arrays `X` and `y` as you did in Homework 0, and then cast each `X_batch` as a `Tensor`, and one hot encode `y_batch` and cast as a `Tensor`. While last time we derived the backpropagation equations for this two-layer ReLU network directly, this time we will be using our auto-differentiation engine to compute the gradients generically by calling the `.backward()` method of the `Tensor` class. For each mini-batch, after calling `.backward()`, you should compute the updated values for `W1` and `W2` in `numpy`, and then create new `Tensors` for `W1` and `W2` with these `numpy` values. Your solution should return the final `W1` and `W2` `Tensors`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHKC82ex9wFg",
    "outputId": "5a319557-624e-45be-e839-3e1d30efb6f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.15, pytest-7.2.0, pluggy-1.0.0\n",
      "rootdir: /home/hujunhao/dlsys/hw1\n",
      "collected 21 items / 20 deselected / 1 selected                                \u001b[0m\n",
      "\n",
      "tests/test_autograd_hw.py \u001b[32m.\u001b[0m\u001b[32m                                              [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m20 deselected\u001b[0m\u001b[32m in 2.92s\u001b[0m\u001b[32m =======================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pytest -l -k \"nn_epoch_ndl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVt8QXrcfee3",
    "outputId": "fad588b5-4f02-4219-84c1-6c9e30be1650"
   },
   "outputs": [],
   "source": [
    "!python3 -m mugrade submit 'YOUR_GRADER_KEY_HERE' -k \"nn_epoch_ndl\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw1_combined.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dlsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "01d030b275ed9843a755dee8127d907d89dd7b57b2b543c3ab64e40d27b5d77b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
